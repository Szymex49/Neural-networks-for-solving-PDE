{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ahkiD46XM02"
      },
      "source": [
        "# Solving Partial Differential Equations with Neural Networks\n",
        "\n",
        "##### Authors: Szymon Malec (262276) & Damian Szuster (262229)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9rjF2r-YBGQ"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej9hgKtnYFoH"
      },
      "source": [
        "Partial differential equations (PDE) appear in numerous fields of science i.a. in mathematics, physics or chemistry. Their task is to describe change of some quantities by using derivatives of given function. Unfortunately, most of them do not have analytical solution. Thus, lots of numerical methods have been derived. One of them, perhaps less known, is solving PDEs with a help of neural networks. That is the main topic of our report.\n",
        "\n",
        "This assignment consists of five parts. The first one includes some theory about neural networks. The second chapter describes partaial differential equations and model parameters chosen for the simulations. Chapters 4 and 5 contain method implementation in Python programming language and analysis of three of its properties: consistency, convergence and stability. After all of these parts some conclusions will be drawn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D_wXf-oYHIX"
      },
      "source": [
        "## 2. Neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7wbDnjOfVar"
      },
      "source": [
        "#### 2.1. Introduction to Neural Networks\n",
        "\n",
        "Neural networks, inspired by the biological neural networks of the human brain, are a subset of machine learning techniques designed to recognize patterns. They consist of interconnected layers of nodes (or neurons) that process data and learn to make predictions or decisions without explicit programming.\n",
        "\n",
        "#### 2.2. Basic Structure of Neural Networks\n",
        "\n",
        "A neural network typically consists of an input layer, one or more hidden layers, and an output layer:\n",
        "\n",
        "- **Input Layer:** This layer receives the raw input data. Each neuron in the input layer represents a feature of the input data.\n",
        "- **Hidden Layers:** These intermediate layers transform the input into something the output layer can use. They apply weights to the inputs and pass the results through activation functions to capture non-linear relationships.\n",
        "- **Output Layer:** This layer produces the final output, which can be a single value or a vector of values, depending on the problem.\n",
        "\n",
        "#### 2.3. Training Neural Networks\n",
        "\n",
        "The process of training a neural network involves the following steps:\n",
        "\n",
        "1. **Forward Propagation:** Input data is passed through the network, layer by layer, until an output is generated.\n",
        "2. **Loss Calculation:** The difference between the predicted output and the actual target is quantified using a loss function.\n",
        "3. **Backward Propagation:** The network adjusts the weights of the connections to minimize the loss. This is done using algorithms such as gradient descent, which calculates the gradient of the loss function with respect to each weight.\n",
        "4. **Iteration:** The process is repeated for many iterations, gradually improving the accuracy of the network.\n",
        "\n",
        "#### 2.4. Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into the network, allowing it to model complex relationships. Common activation functions include:\n",
        "\n",
        "- **Sigmoid:** Outputs values between 0 and 1, used for binary classification.\n",
        "- **ReLU (Rectified Linear Unit):** Outputs the input directly if positive; otherwise, it outputs zero. This function helps to mitigate the vanishing gradient problem.\n",
        "- **Tanh:** Outputs values between -1 and 1, used for centering data.\n",
        "\n",
        "#### 2.5. Neural Networks for Solving PDEs\n",
        "\n",
        "Partial differential equations (PDEs) are equations that involve rates of change with respect to continuous variables. Solving PDEs is crucial in various fields such as physics, engineering, and finance. Traditional methods for solving PDEs, such as finite element methods, can be computationally intensive.\n",
        "\n",
        "Neural networks provide an alternative approach to solving PDEs through the following methods:\n",
        "\n",
        "- **Physics-Informed Neural Networks (PINNs):** These networks incorporate physical laws described by PDEs into the loss function. The network is trained not only on data but also on the underlying physical principles, ensuring that the solution respects the PDE.\n",
        "- **Deep Galerkin Method (DGM):** This method approximates the solution to PDEs using deep neural networks. It leverages the universal approximation capability of neural networks to represent complex solutions.\n",
        "\n",
        "#### 2.6. Advantages of Neural Networks for PDEs\n",
        "\n",
        "- **Flexibility:** Neural networks can approximate complex functions and handle high-dimensional problems that are challenging for traditional methods.\n",
        "- **Data Efficiency:** PINNs and similar approaches can leverage available data more effectively by incorporating physical laws into the learning process.\n",
        "- **Parallelization:** Neural network training can be parallelized, taking advantage of modern high-performance computing resources.\n",
        "\n",
        "#### 2.7. Challenges and Future Directions\n",
        "\n",
        "Despite their advantages, neural networks for PDEs face several challenges:\n",
        "\n",
        "- **Training Complexity:** Training neural networks to solve PDEs can be computationally expensive and requires careful tuning of hyperparameters.\n",
        "- **Generalization:** Ensuring that the network generalizes well to unseen data and different boundary conditions is a significant challenge.\n",
        "- **Interpretability:** Neural network models are often seen as black boxes, making it difficult to interpret the solution process.\n",
        "\n",
        "Future research is directed towards improving training algorithms, developing more interpretable models, and combining neural networks with traditional numerical methods for hybrid approaches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBvkmlzpfWqL"
      },
      "source": [
        "## 3. Model parametrization and choice of equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNLZbO1gCJD3"
      },
      "source": [
        "#### 3.1. Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CghJmaU7fjzY"
      },
      "source": [
        "#### 3.2. Equations\n",
        "\n",
        "To check the quality of our model we have chosen two partial differential equations: wave and Burgers equation. The choice of them is not random: we would like to consider models that have (wave) or do not have (Burgers) analytical solution.\n",
        "\n",
        "Let us begin with the wave equation. It is a second-order linear PDE applied to model wavelike phenomena, e.g. small-amplitude oscilations near equilibrium. In this project we will consider the wave equation in one space dimension:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial^{2}g}{\\partial t^{2}} = c^{2}\\frac{\\partial^{2}g}{\\partial x^{2}},\n",
        "$$\n",
        "\n",
        "where $c$ is wave speed. Additionally, some initial conditions are given:\n",
        "\n",
        "$$\n",
        "g(x,0)=\\phi(x),\n",
        "$$\n",
        "$$\n",
        "g_{t}(x,0)=\\psi(x).\n",
        "$$\n",
        "\n",
        "It is also possible in many ways to describe other boundary conditions. In our case we will use Dirichlet condition. It describes how endpoints of our wave move:\n",
        "\n",
        "$$\n",
        "g(0,t) = \\mu(t),\n",
        "$$\n",
        "\n",
        "$$\n",
        "g(N,t) = \\nu(t).\n",
        "$$\n",
        "\n",
        "We say that endpoints are fixed when $\\mu = \\nu = 0.$\n",
        "\n",
        "The other equation we will use in simulations is Burgers' equation which is one of the fundamental PDEs. This convection-diffusion equation describes phenomena occuring in traffic flow, fluid dynamics etc. As in previous case, we are going to consider Burgers' equation in one space dimension:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial g}{\\partial t} + g \\frac{\\partial g}{\\partial x} = \\nu \\frac{\\partial^{2} g}{\\partial x^{2}},\n",
        "$$\n",
        "\n",
        "where $\\nu$ is a diffusion coeficient. The initial conditions are described in similar form as in wave equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jVItWvqfkCQ"
      },
      "source": [
        "## 4. Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EMxpzFXfpw7"
      },
      "source": [
        "For the implementation we used Python programming language. Some of python packages turned out to be very useful, especially well-known `numpy` library but also `autograd` which provides functions for numerical gradient calculation. Therefore, let's import necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "import autograd.numpy as np\n",
        "from autograd import grad, elementwise_grad\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firstly we define two activation functions, which we'll use in neural networks. These are ReLu and sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's create the function which returns the output of the neural network. The function below calculates values in all neurons layer by layer ending with the output layer. It needs weights of the network to be specified and passed as numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def neural_network(x, weights, activation_function=sigmoid):\n",
        "    for W in weights[:-1]:\n",
        "        x = np.vstack([np.ones(x.shape[1]), x])\n",
        "        x = activation_function(W @ x)\n",
        "    x = np.vstack([np.ones(x.shape[1]), x])\n",
        "    x = weights[-1] @ x\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An inherent element of neural network is the cost (loss) function, which should be minimized through the training process. The right choice of the cost is an important step. In case of Physical Informed Neural Networks the cost function is assumed to be the square of the differential equation. Notice that, if we put all the elements on one side of the equation, we can represent the equation as\n",
        "$$\n",
        "f\\left(g, x_1, \\, \\dots \\, , x_N, \\frac{\\partial g}{\\partial x_1}, \\dots , \\frac{\\partial g}{\\partial x_N}, \\frac{\\partial g}{\\partial x_1\\partial x_2}, \\, \\dots \\, , \\frac{\\partial^n }{\\partial x_N^n} \\right) = 0\n",
        "$$\n",
        "for the function $g = g(x_1,\\dots,x_N)$ of $N$ variables. We may treat the above as an error. Thus the cost function can be expressed as\n",
        "$$ C(x, W) = f^2. $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cost(X, g_t, equation, weights):\n",
        "    return np.mean(equation(X, g_t, weights)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally the function which solves the equation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve(equation, g_t, X, layers, epochs, learning_rate=0.001):\n",
        "\n",
        "    cost_grad = elementwise_grad(cost, 3)\n",
        "    weights = [np.random.randn(layers[layer + 1], layers[layer] + 1) for layer in range(len(layers) - 1)]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        dW = cost_grad(X, g_t, equation, weights)\n",
        "        for w in range(len(weights)):\n",
        "            weights[w] -= learning_rate * dW[w]\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ACpZQQfqEr"
      },
      "source": [
        "## 5. Properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uWXPrOefsmf"
      },
      "source": [
        "#### 5.1. Consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MxMUWxHfs4M"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVyBcQtKftKf"
      },
      "source": [
        "#### 5.2. Convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_8yFLsVftZB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c3-mnP7ftrN"
      },
      "source": [
        "#### 5.3. Stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oIL2E0Vft4g"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjrnWZ07gTtU"
      },
      "source": [
        "## 6. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rMmNIoOgaNa"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGhyQspICkUO"
      },
      "source": [
        "## 7. Bibliography"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPoZwdRYCl0U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
